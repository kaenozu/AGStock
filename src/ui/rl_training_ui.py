import time
import pandas as pd
import numpy as np
import streamlit as st
import logging

from src.rl.environment import TradingEnvironment
from src.rl.agent import DQNAgent

logger = logging.getLogger(__name__)


def generate_demo_market_data(length=1000):
    """Generate synthetic market data for training demo"""
    x = np.linspace(0, 100, length)
    # Trend + Sine Wave + Noise
    prices = 100 + (x * 0.5) + 10 * np.sin(x * 0.2) + np.random.normal(0, 2, length)

    df = pd.DataFrame(
        {
            "Open": prices,
            "High": prices + 2,
            "Low": prices - 2,
            "Close": prices,
            "Volume": np.random.randint(1000, 5000, length),
        }
    )
    return df


def render_rl_training_ui():
    """Renders the AI Training Gym UI"""
    st.header("ğŸ‹ï¸ AIãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ  (RL Gym)")
    st.caption("å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è‚²æˆãƒ—ãƒ­ã‚»ã‚¹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")

    col1, col2 = st.columns([1, 3])

    with col1:
        st.subheader("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š")
        episodes = st.slider("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°", min_value=5, max_value=100, value=20)
        initial_balance = st.number_input("åˆæœŸè³‡é‡‘", value=100000)
        transaction_cost = st.number_input("å–å¼•ã‚³ã‚¹ãƒˆ (%)", value=0.001, format="%.4f")

        start_btn = st.button("ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹", type="primary")

        st.info("â€»ãƒ‡ãƒ¢ç”¨ã«ç”Ÿæˆã•ã‚ŒãŸå¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

    with col2:
        if start_btn:
            run_training_session(episodes, initial_balance, transaction_cost)
        else:
            st.markdown(
                """
### ã“ã“ã§ä½•ãŒã§ãã‚‹ï¼Ÿ

            AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆDQNãƒ¢ãƒ‡ãƒ«ï¼‰ãŒã€å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã§**ã€Œè©¦è¡ŒéŒ¯èª¤ã€**ã—ãªãŒã‚‰æˆé•·ã™ã‚‹æ§˜å­ã‚’è¦³å¯Ÿã§ãã¾ã™ã€‚

            - **Reward (å ±é…¬)**: é«˜ã„ã»ã©è‰¯ã„è¡Œå‹•ã‚’ã¨ã£ã¦ã„ã¾ã™ã€‚
            - **Epsilon (æ¢ç´¢ç‡)**: åˆã‚ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«å‹•ãã€å¾ã€…ã«å­¦ç¿’ã—ãŸçŸ¥è­˜ã‚’ä½¿ã†ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

            å·¦å´ã®ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€AIã®æˆé•·ã‚’è¦‹å®ˆã‚Šã¾ã—ã‚‡ã†ï¼
            """
            )


def run_training_session(episodes, initial_balance, transaction_cost):
    st.subheader("ğŸ“Š ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€²æ—")

    # 1. Init Data & Env
    with st.spinner("ç’°å¢ƒã‚’æ§‹ç¯‰ä¸­..."):
        df = generate_demo_market_data()
        env = TradingEnvironment(df, initial_balance=initial_balance, transaction_cost_pct=transaction_cost)

        # Init Agent
        try:
            agent = DQNAgent(env.state_size, env.action_space_size)
        except Exception as e:
            st.error(f"Agent Initialization Error: {e}")
            return

    # 2. UI Elements for updates
    progress_bar = st.progress(0)
    status_text = st.empty()

    # Charts
    chart_placeholder = st.empty()
    metrics_placeholder = st.empty()

    rewards_history = []
    portfolio_history = []

    # 3. Training Loop
    for e in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)

            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

        # Experience Replay (Train)
        agent.replay()

        # Log Stats
        rewards_history.append(total_reward)
        final_portfolio = info["portfolio_value"]
        portfolio_history.append(final_portfolio)

        # Update UI
        progress = (e + 1) / episodes
        progress_bar.progress(progress)

        status_text.markdown(f"**Episode {e + 1}/{episodes}** | Epsilon: `{agent.epsilon:.2f}`")

        # Update Chart (Dual Axis ideally, but Streamlit line_chart is simple)
        # We plot Rewards
        chart_data = pd.DataFrame({"Reward (å­¦ç¿’æˆæœ)": rewards_history})
        chart_placeholder.line_chart(chart_data)

        # Metrics
        latest_pnl = (final_portfolio - initial_balance) / initial_balance * 100
        metrics_placeholder.markdown(
            f"""
        - **Last Reward**: {total_reward:.2f}
        - **Current Portfolio**: {final_portfolio:,.0f} (+{latest_pnl:.2f}%)
        """
        )

        # Slight delay for visual effect
        time.sleep(0.1)

    st.success("ğŸ‰ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼ ãƒ¢ãƒ‡ãƒ«ãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚")

    # Save Model
    try:
        agent.save("models/rl_gym_trained.pth")
        st.caption("ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: `models/rl_gym_trained.pth`")
    except Exception as e:
        st.error(f"ä¿å­˜å¤±æ•—: {e}")
