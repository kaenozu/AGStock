import streamlit as st
import os
import json
from datetime import datetime
import pandas as pd
from src.core.archive_manager import ArchiveManager
from src.core.knowledge_extractor import KnowledgeExtractor
from src.ui.design_system import apply_premium_style


def render_archive_explorer():
    #     """
    #     The Eternal Archive Explorer - Browse the complete history of AI decisions.
    #         apply_premium_style()
    #         st.title("ğŸ“š The Eternal Archive")
    #     st.markdown(
    #             <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin-bottom: 25px;'>
    #         <h2 style='color: white; margin: 0;'>æ°¸é ã®è¨˜éŒ²åº«</h2>
    #         <p style='color: #e5e7eb; margin: 5px 0 0 0;'>å…¨ã¦ã®æ„æ€æ±ºå®šã®å®Œå…¨ãªè¨˜éŒ²ã¨ã€ãã“ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸæ™®éçš„ãªçŸ¥è¦‹</p>
    #     </div>
    #     """,
    unsafe_allow_html = (True,)
    #     )
    archive = ArchiveManager()
    #     extractor = KnowledgeExtractor()
    tab1, tab2, tab3, tab4 = st.tabs(
        ["ğŸ“– æ±ºå®šã®å±¥æ­´", "ğŸ§  çŸ¥è¦‹ã®æŠ½å‡º", "ğŸ”® äºˆæ¸¬ã®æ¤œè¨¼", "ğŸ“Š çµ±è¨ˆåˆ†æ"]
    )
    with tab1:
        st.subheader("æ„æ€æ±ºå®šã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³")


# Date range selector
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("é–‹å§‹æ—¥", datetime.now().replace(day=1))
    #         with col2:
    end_date = st.date_input("çµ‚äº†æ—¥", datetime.now())
# Load decisions
decisions = load_decisions_in_range(archive, start_date, end_date)
if decisions:
    st.metric("æœŸé–“å†…ã®æ±ºå®šæ•°", len(decisions))
# Display decisions
# for dec in reversed(decisions[-50:]):  # Latest 50
with st.expander(
    f"{dec.get('timestamp', '')[:10]} | {dec.get('ticker')} | {dec.get('decision')}"
):
    col_a, col_b = st.columns(2)
    #                         with col_a:
    #                             st.write("**æ±ºå®šè©³ç´°**")
    #                         st.write(f"ä¿¡é ¼åº¦: {dec.get('confidence', 0):.1%}")
    #                         st.write(f"ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ : {dec.get('context', {}).get('paradigm', 'N/A')}")
    #                         st.write(f"ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹: {dec.get('deliberation', {}).get('consensus_strength', 0):.1%}")
    #                         with col_b:
    #                             st.write("**å‚åŠ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**")
    #                         agents = dec.get("deliberation", {}).get("agents_involved", [])
    #                         for agent in agents:
    #                             st.write(f"- {agent}")
    #                         st.write("**å¸‚å ´ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**")
    st.json(dec.get("context", {}).get("market_data", {}))
    #         else:
    #             st.info("æŒ‡å®šæœŸé–“å†…ã«è¨˜éŒ²ã•ã‚ŒãŸæ±ºå®šã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    #         with tab2:
    #             st.subheader("æ™®éçš„ãªæ³•å‰‡ã®æŠ½å‡º")
    #             if st.button("ğŸ§  çŸ¥è¦‹ã‚’æŠ½å‡º (æœ€æ–°90æ—¥)"):
    #                 with st.spinner("AIãŒéå»ã®æ±ºå®šã‚’åˆ†æä¸­..."):
    decisions = load_decisions_in_range(
        archive, datetime.now().replace(day=1), datetime.now()
    )
    if len(decisions) > 10:
        insights = extractor.extract_universal_laws(decisions)
        if insights.get("universal_laws"):
            st.success(
                f"âœ… {len(insights['universal_laws'])} å€‹ã®æ™®éçš„æ³•å‰‡ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ"
            )
            for law in insights["universal_laws"]:
                st.info(
                    f"ğŸ“– **{law.get('law')}**\n\nä¿¡é ¼åº¦: {law.get('confidence', 0):.0%} | è¨¼æ‹ æ•°: {law.get('evidence_count', 0)}"
                )
            if insights.get("meta_insights"):
                pass
#                                 st.markdown("""" ğŸ¯ æˆ¦ç•¥çš„æ¨å¥¨")
#                             st.write(insights["meta_insights"])
#                     else:
#     pass
#                         st.warning("ååˆ†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
#                 else:
#     pass
#                     st.warning("åˆ†æã«ã¯æœ€ä½10ä»¶ã®æ±ºå®šè¨˜éŒ²ãŒå¿…è¦ã§ã™ã€‚")
# # Display existing knowledge
#         st.markdown("---")
#         st.subheader("è“„ç©ã•ã‚ŒãŸçŸ¥è¦‹")
#             knowledge_path = os.path.join(archive.archive_dir, "knowledge", "universal_patterns.json")
#         if os.path.exists(knowledge_path):
#     pass
#             with open(knowledge_path, "r", encoding="utf-8") as f:
#     pass
#                 knowledge = json.load(f)
#                 st.write(f"**æœ€çµ‚æ›´æ–°**: {knowledge.get('last_updated', 'N/A')[:10]}")
#                 laws = knowledge.get("universal_laws", [])
#             if laws:
#     pass
#                 for law in laws[-10:]:  # Latest 10
#                     st.write(f"- {law.get('law')} (ä¿¡é ¼åº¦: {law.get('confidence', 0):.0%})")
#         else:
#     pass
#             st.info("ã¾ã çŸ¥è¦‹ãŒæŠ½å‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä¸Šã®ãƒœã‚¿ãƒ³ã‹ã‚‰æŠ½å‡ºã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
#         with tab3:
#     pass
#             st.subheader("äºˆæ¸¬ã®æ¤œè¨¼")
#             if st.button("ğŸ” äºˆæ¸¬ã‚’æ¤œè¨¼"):
#     pass
#                 with st.spinner("éå»ã®äºˆæ¸¬ã‚’æ¤œè¨¼ä¸­..."):
#     pass
#                     # Mock market data for verification
#                 verification_results = archive.verify_predictions({})
#                     col1, col2, col3 = st.columns(3)
#                 with col1:
#     pass
#                     st.metric("æ¤œè¨¼æ¸ˆã¿äºˆæ¸¬", verification_results.get("verified_count", 0))
#                 with col2:
#     pass
#                     st.metric("çš„ä¸­ç‡", f"{verification_results.get('accuracy_rate', 0):.1%}")
#                 with col3:
#     pass
#                     st.metric("å¹³å‡èª¤å·®", f"{verification_results.get('average_error', 0):.2f}")
#             st.info("äºˆæ¸¬æ¤œè¨¼æ©Ÿèƒ½ã¯ã€æ™‚é–“çµŒéå¾Œã«è‡ªå‹•çš„ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚")
#         with tab4:
#     pass
#             st.subheader("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–çµ±è¨ˆ")
# # Calculate statistics
#         stats = calculate_archive_statistics(archive)
#             col1, col2, col3, col4 = st.columns(4)
#         with col1:
#     pass
#             st.metric("ç·æ±ºå®šæ•°", stats.get("total_decisions", 0))
#         with col2:
#     pass
#             st.metric("ç·äºˆæ¸¬æ•°", stats.get("total_predictions", 0))
#         with col3:
#     pass
#             st.metric("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚µã‚¤ã‚º", f"{stats.get('archive_size_mb', 0):.1f} MB")
#         with col4:
#     pass
#             st.metric("æœ€å¤ã®è¨˜éŒ²", stats.get("oldest_record", "N/A")[:10])
# # Paradigm distribution
#         if stats.get("paradigm_distribution"):
#     pass
#             st.markdown("""" ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ åˆ†å¸ƒ")
paradigm_df = pd.DataFrame(
    list(stats["paradigm_distribution"].items()), columns=["ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ", "ä»¶æ•°"]
)
st.bar_chart(paradigm_df.set_index("ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ "))


def load_decisions_in_range(archive: ArchiveManager, start_date, end_date) -> list:
    pass


#     """Loads decisions within date range."""
decisions = []
for root, dirs, files in os.walk(archive.decisions_dir):
    for file in files:
        if not file.endswith(".json"):
            pass
        continue
        filepath = os.path.join(root, file)
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            dec = json.load(f)
            dec_date = datetime.fromisoformat(dec["timestamp"]).date()
        if start_date <= dec_date <= end_date:
            decisions.append(dec)
    except Exception:
        continue


#         return sorted(decisions, key=lambda x: x["timestamp"])
def calculate_archive_statistics(archive: ArchiveManager) -> dict:
    #     """Calculates overall archive statistics."""
    stats = {
        "total_decisions": 0,
        "total_predictions": 0,
        "archive_size_mb": 0.0,
        "oldest_record": None,
        "paradigm_distribution": {},
    }


# Count decisions
for root, dirs, files in os.walk(archive.decisions_dir):
    for file in files:
        if file.endswith(".json"):
            stats["total_decisions"] += 1
            #                     filepath = os.path.join(root, file)
            stats["archive_size_mb"] += os.path.getsize(filepath) / (1024 * 1024)
#                     try:
#                         with open(filepath, "r", encoding="utf-8") as f:
#                             dec = json.load(f)
#                         timestamp = dec.get("timestamp")
#                     if not stats["oldest_record"] or timestamp < stats["oldest_record"]:
#                         stats["oldest_record"] = timestamp
#                         paradigm = dec.get("context", {}).get("paradigm", "UNKNOWN")
#                     stats["paradigm_distribution"][paradigm] = stats["paradigm_distribution"].get(paradigm, 0) + 1
#                 except Exception:
#                     continue
# Count predictions
for root, dirs, files in os.walk(archive.predictions_dir):
    for file in files:
        if file.startswith("pred_"):
            stats["total_predictions"] += 1
#         return stats
